# Ollama Setup Guide - à¸„à¸¹à¹ˆà¸¡à¸·à¸­à¸•à¸´à¸”à¸•à¸±à¹‰à¸‡ Local AI

> à¸„à¸¹à¹ˆà¸¡à¸·à¸­à¸¥à¸°à¹€à¸­à¸µà¸¢à¸”à¸ªà¸³à¸«à¸£à¸±à¸šà¸•à¸´à¸”à¸•à¸±à¹‰à¸‡à¹à¸¥à¸°à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸² Ollama à¹€à¸à¸·à¹ˆà¸­à¹ƒà¸Šà¹‰à¸à¸±à¸š PMQA ISOC System

---

## ğŸ“‹ à¸‚à¹‰à¸­à¸à¸³à¸«à¸™à¸”à¸£à¸°à¸šà¸š (System Requirements)

### âš™ï¸ à¸‚à¸±à¹‰à¸™à¸•à¹ˆà¸³ (Minimum)
- **OS**: Windows 10/11, Ubuntu 20.04+, macOS 11+
- **RAM**: 8 GB
- **Storage**: 10 GB à¸§à¹ˆà¸²à¸‡ (à¸ªà¸³à¸«à¸£à¸±à¸šà¹‚à¸¡à¹€à¸”à¸¥ 8B)
- **Internet**: à¸ªà¸³à¸«à¸£à¸±à¸šà¸”à¸²à¸§à¸™à¹Œà¹‚à¸«à¸¥à¸”à¸„à¸£à¸±à¹‰à¸‡à¹à¸£à¸

### ğŸš€ à¹à¸™à¸°à¸™à¸³ (Recommended)
- **OS**: Windows 11 Pro, Ubuntu 22.04 LTS, macOS 14+
- **RAM**: 16 GB+ (32 GB à¸ªà¸³à¸«à¸£à¸±à¸šà¹‚à¸¡à¹€à¸”à¸¥à¹ƒà¸«à¸à¹ˆ)
- **GPU**: NVIDIA RTX 3060+ (6GB VRAM)
- **Storage**: 50 GB+ à¸§à¹ˆà¸²à¸‡
- **CPU**: 8 cores+

---

## ğŸªŸ à¸à¸²à¸£à¸•à¸´à¸”à¸•à¸±à¹‰à¸‡à¸šà¸™ Windows

### à¸§à¸´à¸˜à¸µà¸—à¸µà¹ˆ 1: à¸•à¸´à¸”à¸•à¸±à¹‰à¸‡à¸”à¹‰à¸§à¸¢ Installer (à¹à¸™à¸°à¸™à¸³)

#### à¸‚à¸±à¹‰à¸™à¸•à¸­à¸™à¸—à¸µà¹ˆ 1: à¸”à¸²à¸§à¸™à¹Œà¹‚à¸«à¸¥à¸” Ollama

1. à¹€à¸›à¸´à¸”à¹€à¸§à¹‡à¸š: https://ollama.com/download/windows
2. à¸„à¸¥à¸´à¸ **Download for Windows**
3. Save file: `OllamaSetup.exe`

#### à¸‚à¸±à¹‰à¸™à¸•à¸­à¸™à¸—à¸µà¹ˆ 2: à¸•à¸´à¸”à¸•à¸±à¹‰à¸‡

```powershell
# Double-click OllamaSetup.exe
# à¸«à¸£à¸·à¸­à¹ƒà¸Šà¹‰ Command Line:
.\OllamaSetup.exe /S
```

> **à¸«à¸¡à¸²à¸¢à¹€à¸«à¸•à¸¸:** à¸à¸²à¸£à¸•à¸´à¸”à¸•à¸±à¹‰à¸‡à¸ˆà¸°à¹ƒà¸Šà¹‰à¹€à¸§à¸¥à¸² 2-3 à¸™à¸²à¸—à¸µ

#### à¸‚à¸±à¹‰à¸™à¸•à¸­à¸™à¸—à¸µà¹ˆ 3: à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸à¸²à¸£à¸•à¸´à¸”à¸•à¸±à¹‰à¸‡

```powershell
ollama --version
# Output: ollama version 0.x.x
```

---

### à¸§à¸´à¸˜à¸µà¸—à¸µà¹ˆ 2: à¸•à¸´à¸”à¸•à¸±à¹‰à¸‡à¸”à¹‰à¸§à¸¢ winget

```powershell
# à¹€à¸›à¸´à¸” PowerShell as Administrator
winget install Ollama.Ollama

# à¸£à¸­à¸•à¸´à¸”à¸•à¸±à¹‰à¸‡à¹€à¸ªà¸£à¹‡à¸ˆ à¸ˆà¸²à¸à¸™à¸±à¹‰à¸™à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š
ollama --version
```

---

### à¸‚à¸±à¹‰à¸™à¸•à¸­à¸™à¸—à¸µà¹ˆ 4: à¸”à¸²à¸§à¸™à¹Œà¹‚à¸«à¸¥à¸”à¹‚à¸¡à¹€à¸”à¸¥

```powershell
# à¹‚à¸¡à¹€à¸”à¸¥à¹à¸™à¸°à¸™à¸³à¸ªà¸³à¸«à¸£à¸±à¸šà¸ à¸²à¸©à¸²à¹„à¸—à¸¢ (à¹€à¸¥à¸·à¸­à¸ 1 à¸•à¸±à¸§)

# 1. Qwen 2.5 14B (à¸ à¸²à¸©à¸²à¹„à¸—à¸¢à¸”à¸µà¸—à¸µà¹ˆà¸ªà¸¸à¸”) - à¹à¸™à¸°à¸™à¸³! ğŸŒŸ
ollama pull qwen2.5:14b
# à¸‚à¸™à¸²à¸”: 9 GB, à¸•à¹‰à¸­à¸‡ RAM 16 GB

# 2. Llama 3.1 8B (à¹€à¸£à¹‡à¸§ à¹ƒà¸Šà¹‰ RAM à¸™à¹‰à¸­à¸¢)
ollama pull llama3.1:8b
# à¸‚à¸™à¸²à¸”: 4.7 GB, à¸•à¹‰à¸­à¸‡ RAM 8 GB

# 3. Mistral 7B (à¸ªà¸¡à¸”à¸¸à¸¥à¸”à¸µ)
ollama pull mistral:7b
# à¸‚à¸™à¸²à¸”: 4.1 GB, à¸•à¹‰à¸­à¸‡ RAM 8 GB
```

> **â±ï¸ à¹€à¸§à¸¥à¸²à¸”à¸²à¸§à¸™à¹Œà¹‚à¸«à¸¥à¸”:** 10-30 à¸™à¸²à¸—à¸µ (à¸‚à¸¶à¹‰à¸™à¸à¸±à¸šà¸„à¸§à¸²à¸¡à¹€à¸£à¹‡à¸§à¹€à¸™à¹‡à¸•)

---

### à¸‚à¸±à¹‰à¸™à¸•à¸­à¸™à¸—à¸µà¹ˆ 5: à¹€à¸£à¸´à¹ˆà¸¡à¹ƒà¸Šà¹‰à¸‡à¸²à¸™ Ollama

```powershell
# à¹€à¸£à¸´à¹ˆà¸¡ Ollama server
ollama serve
```

**Output:**
```
Listening on 127.0.0.1:11434 (version 0.x.x)
time=2026-01-21T15:00:00.000+07:00 level=INFO msg="Ollama started"
```

> **ğŸ’¡ Tip:** Ollama à¸ˆà¸°à¸£à¸±à¸™à¹€à¸›à¹‡à¸™ background service à¸­à¸±à¸•à¹‚à¸™à¸¡à¸±à¸•à¸´à¸«à¸¥à¸±à¸‡à¸•à¸´à¸”à¸•à¸±à¹‰à¸‡
> à¸„à¸³à¸ªà¸±à¹ˆà¸‡ `ollama serve` à¹„à¸¡à¹ˆà¸ˆà¸³à¹€à¸›à¹‡à¸™à¸•à¹‰à¸­à¸‡à¸£à¸±à¸™à¸—à¸¸à¸à¸„à¸£à¸±à¹‰à¸‡

---

### à¸‚à¸±à¹‰à¸™à¸•à¸­à¸™à¸—à¸µà¹ˆ 6: à¸—à¸”à¸ªà¸­à¸šà¹‚à¸¡à¹€à¸”à¸¥

```powershell
# à¸—à¸”à¸ªà¸­à¸š Chat
ollama run qwen2.5:14b

# à¸à¸´à¸¡à¸à¹Œà¸„à¸³à¸–à¸²à¸¡:
>>> à¸ªà¸§à¸±à¸ªà¸”à¸µà¸„à¸£à¸±à¸š à¸„à¸¸à¸“à¸Šà¸·à¹ˆà¸­à¸­à¸°à¹„à¸£
à¸ªà¸§à¸±à¸ªà¸”à¸µà¸„à¸£à¸±à¸š! à¸œà¸¡à¸„à¸·à¸­ Qwen à¸œà¸¹à¹‰à¸Šà¹ˆà¸§à¸¢ AI à¸—à¸µà¹ˆà¸à¸±à¸’à¸™à¸²à¹‚à¸”à¸¢ Alibaba Cloud...

# à¸à¸” Ctrl+D à¹€à¸à¸·à¹ˆà¸­à¸­à¸­à¸
```

---

## ğŸ§ à¸à¸²à¸£à¸•à¸´à¸”à¸•à¸±à¹‰à¸‡à¸šà¸™ Linux (Ubuntu/Debian)

```bash
# à¸•à¸´à¸”à¸•à¸±à¹‰à¸‡ Ollama
curl -fsSL https://ollama.com/install.sh | sh

# à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š
ollama --version

# à¸”à¸²à¸§à¸™à¹Œà¹‚à¸«à¸¥à¸”à¹‚à¸¡à¹€à¸”à¸¥
ollama pull qwen2.5:14b

# à¹€à¸£à¸´à¹ˆà¸¡ service (à¸ˆà¸°à¸£à¸±à¸™à¸­à¸±à¸•à¹‚à¸™à¸¡à¸±à¸•à¸´)
systemctl status ollama

# à¸—à¸”à¸ªà¸­à¸š
ollama run qwen2.5:14b
```

---

## ğŸ à¸à¸²à¸£à¸•à¸´à¸”à¸•à¸±à¹‰à¸‡à¸šà¸™ macOS

```bash
# à¸”à¸²à¸§à¸™à¹Œà¹‚à¸«à¸¥à¸” .dmg à¸ˆà¸²à¸ https://ollama.com/download/mac
# à¸«à¸£à¸·à¸­à¹ƒà¸Šà¹‰ Homebrew:
brew install ollama

# à¸”à¸²à¸§à¸™à¹Œà¹‚à¸«à¸¥à¸”à¹‚à¸¡à¹€à¸”à¸¥
ollama pull qwen2.5:14b

# à¸—à¸”à¸ªà¸­à¸š
ollama run qwen2.5:14b
```

---

## ğŸ”§ à¸à¸²à¸£à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸² (Configuration)

### 1. à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™ Default Port (à¸–à¹‰à¸²à¸•à¹‰à¸­à¸‡à¸à¸²à¸£)

**Windows:**
```powershell
# Set environment variable
[Environment]::SetEnvironmentVariable("OLLAMA_HOST", "0.0.0.0:11435", "User")

# à¸£à¸µà¸ªà¸•à¸²à¸£à¹Œà¸— Ollama
taskkill /F /IM ollama.exe
ollama serve
```

**Linux:**
```bash
# à¹à¸à¹‰à¹„à¸‚ service file
sudo nano /etc/systemd/system/ollama.service

# à¹€à¸à¸´à¹ˆà¸¡à¸šà¸£à¸£à¸—à¸±à¸”:
Environment="OLLAMA_HOST=0.0.0.0:11435"

# à¸£à¸µà¹‚à¸«à¸¥à¸”
sudo systemctl daemon-reload
sudo systemctl restart ollama
```

---

### 2. à¹€à¸›à¸´à¸”à¹ƒà¸«à¹‰à¹€à¸‚à¹‰à¸²à¸–à¸¶à¸‡à¸ˆà¸²à¸à¹€à¸„à¸£à¸·à¹ˆà¸­à¸‡à¸­à¸·à¹ˆà¸™ (Remote Access)

```powershell
# Windows: à¹€à¸‹à¹‡à¸• bind to 0.0.0.0
$env:OLLAMA_HOST = "0.0.0.0:11434"
ollama serve

# à¹€à¸›à¸´à¸” Firewall
netsh advfirewall firewall add rule name="Ollama" dir=in action=allow protocol=TCP localport=11434
```

---

### 3. à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸² GPU (NVIDIA)

```bash
# à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š GPU
nvidia-smi

# Ollama à¸ˆà¸°à¹ƒà¸Šà¹‰ GPU à¸­à¸±à¸•à¹‚à¸™à¸¡à¸±à¸•à¸´
# à¹€à¸Šà¹‡à¸„à¸§à¹ˆà¸²à¹ƒà¸Šà¹‰ GPU à¸ˆà¸£à¸´à¸‡à¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆ:
ollama run qwen2.5:14b --verbose
```

---

## ğŸ”— à¹€à¸Šà¸·à¹ˆà¸­à¸¡à¸•à¹ˆà¸­à¸à¸±à¸š PMQA Web App

### à¸‚à¸±à¹‰à¸™à¸•à¸­à¸™à¸—à¸µà¹ˆ 1: Config à¹ƒà¸™ Admin Panel

1. Login à¹€à¸›à¹‡à¸™ **Admin**
2. à¹„à¸› **Admin â†’ AI Settings â†’ Ollama Setup**
3. à¸à¸£à¸­à¸:
   ```
   Ollama Server URL: http://localhost:11434
   Default Model: qwen2.5:14b
   ```
4. à¸„à¸¥à¸´à¸ **Test Connection**
   - âœ… à¸–à¹‰à¸²à¸‚à¸¶à¹‰à¸™ "Connected" â†’ à¸à¸£à¹‰à¸­à¸¡à¹ƒà¸Šà¹‰à¸‡à¸²à¸™
   - âŒ à¸–à¹‰à¸²à¸‚à¸¶à¹‰à¸™ "Failed" â†’ à¹€à¸Šà¹‡à¸„à¸‚à¸±à¹‰à¸™à¸•à¸­à¸™à¸”à¹‰à¸²à¸™à¸¥à¹ˆà¸²à¸‡

---

### à¸‚à¸±à¹‰à¸™à¸•à¸­à¸™à¸—à¸µà¹ˆ 2: Enable Local AI

1. Toggle: **Enable Local AI for all users** â†’ ON
2. Save Settings

---

### à¸‚à¸±à¹‰à¸™à¸•à¸­à¸™à¸—à¸µà¹ˆ 3: User à¹€à¸¥à¸·à¸­à¸ Provider

à¸œà¸¹à¹‰à¹ƒà¸Šà¹‰à¸—à¸¸à¸à¸„à¸™à¸ªà¸²à¸¡à¸²à¸£à¸–:
1. à¹„à¸› **Settings â†’ AI Provider**
2. à¹€à¸¥à¸·à¸­à¸ **Local AI (Ollama)**
3. Save

---

## ğŸš¨ Troubleshooting (à¹à¸à¹‰à¸›à¸±à¸à¸«à¸²)

### à¸›à¸±à¸à¸«à¸² 1: "Connection refused" à¸«à¸£à¸·à¸­ "Ollama offline"

**à¸§à¸´à¸˜à¸µà¹à¸à¹‰:**
```powershell
# Windows: à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸§à¹ˆà¸² Ollama à¸£à¸±à¸™à¸­à¸¢à¸¹à¹ˆà¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆ
tasklist | findstr ollama

# à¸–à¹‰à¸²à¹„à¸¡à¹ˆà¹€à¸ˆà¸­ â†’ à¹€à¸£à¸´à¹ˆà¸¡à¹ƒà¸«à¸¡à¹ˆ
ollama serve

# à¸¥à¸­à¸‡ curl
curl http://localhost:11434/api/tags
```

---

### à¸›à¸±à¸à¸«à¸² 2: "Model not found"

**à¸§à¸´à¸˜à¸µà¹à¸à¹‰:**
```bash
# à¸”à¸¹ models à¸—à¸µà¹ˆà¸¡à¸µ
ollama list

# à¸–à¹‰à¸²à¹„à¸¡à¹ˆà¸¡à¸µ qwen2.5:14b â†’ à¸”à¸²à¸§à¸™à¹Œà¹‚à¸«à¸¥à¸”à¹ƒà¸«à¸¡à¹ˆ
ollama pull qwen2.5:14b
```

---

### à¸›à¸±à¸à¸«à¸² 3: "Out of memory"

**à¸§à¸´à¸˜à¸µà¹à¸à¹‰:**
- à¹ƒà¸Šà¹‰à¹‚à¸¡à¹€à¸”à¸¥à¹€à¸¥à¹‡à¸à¸à¸§à¹ˆà¸²: `llama3.1:8b` à¸«à¸£à¸·à¸­ `mistral:7b`
- à¸›à¸´à¸”à¹‚à¸›à¸£à¹à¸à¸£à¸¡à¸­à¸·à¹ˆà¸™à¸—à¸µà¹ˆà¸à¸´à¸™ RAM
- Upgrade RAM à¹€à¸›à¹‡à¸™ 16GB+

---

### à¸›à¸±à¸à¸«à¸² 4: Response à¸Šà¹‰à¸²à¸¡à¸²à¸ (>30 à¸§à¸´à¸™à¸²à¸—à¸µ)

**à¸§à¸´à¸˜à¸µà¹à¸à¹‰:**
- à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸§à¹ˆà¸²à¸¡à¸µ GPU à¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆ: `nvidia-smi`
- à¹ƒà¸Šà¹‰à¹‚à¸¡à¹€à¸”à¸¥à¹€à¸¥à¹‡à¸à¸à¸§à¹ˆà¸²: `llama3.1:8b`
- à¸›à¸´à¸” programs à¸—à¸µà¹ˆà¹ƒà¸Šà¹‰ GPU à¸­à¸·à¹ˆà¸™à¹†

---

## âš¡ One-Click Setup Script

### à¸ªà¸³à¸«à¸£à¸±à¸š Windows (PowerShell)

```powershell
# à¸”à¸²à¸§à¸™à¹Œà¹‚à¸«à¸¥à¸” script à¸ˆà¸²à¸ Web App:
# Admin â†’ Ollama Setup â†’ Download Setup Script

# à¸«à¸£à¸·à¸­à¸„à¸±à¸”à¸¥à¸­à¸ script à¸™à¸µà¹‰:

# ==========================================
# Ollama Auto Setup Script for Windows
# ==========================================

Write-Host "ğŸš€ Starting Ollama Setup..." -ForegroundColor Green

# 1. Check if Ollama installed
if (Get-Command ollama -ErrorAction SilentlyContinue) {
    Write-Host "âœ… Ollama already installed" -ForegroundColor Green
} else {
    Write-Host "ğŸ“¥ Installing Ollama..." -ForegroundColor Yellow
    winget install Ollama.Ollama -h --accept-source-agreements --accept-package-agreements
    Write-Host "âœ… Ollama installed" -ForegroundColor Green
}

# 2. Pull model
Write-Host "ğŸ“¦ Downloading qwen2.5:14b model..." -ForegroundColor Yellow
ollama pull qwen2.5:14b
Write-Host "âœ… Model downloaded" -ForegroundColor Green

# 3. Start service
Write-Host "ğŸ”„ Starting Ollama service..." -ForegroundColor Yellow
Start-Process ollama -ArgumentList "serve" -WindowStyle Hidden
Start-Sleep -Seconds 3

# 4. Test
Write-Host "ğŸ§ª Testing connection..." -ForegroundColor Yellow
$response = Invoke-RestMethod -Uri "http://localhost:11434/api/tags" -Method Get
if ($response) {
    Write-Host "âœ… Ollama is running!" -ForegroundColor Green
    Write-Host "Models installed:" -ForegroundColor Cyan
    $response.models | ForEach-Object { Write-Host "  - $($_.name)" -ForegroundColor White }
} else {
    Write-Host "âŒ Connection failed" -ForegroundColor Red
}

Write-Host "`nğŸ‰ Setup complete! You can now use Local AI in PMQA app." -ForegroundColor Green
```

**à¸§à¸´à¸˜à¸µà¹ƒà¸Šà¹‰:**
1. à¹€à¸›à¸´à¸” PowerShell as Administrator
2. Copy-Paste script à¸‚à¹‰à¸²à¸‡à¸šà¸™
3. à¸à¸” Enter
4. à¸£à¸­à¸ˆà¸™à¹€à¸ªà¸£à¹‡à¸ˆ (~10-30 à¸™à¸²à¸—à¸µ)

---

### à¸ªà¸³à¸«à¸£à¸±à¸š Linux (Bash)

```bash
#!/bin/bash
# Ollama Auto Setup Script for Linux

echo "ğŸš€ Starting Ollama Setup..."

# 1. Install Ollama
if command -v ollama &> /dev/null; then
    echo "âœ… Ollama already installed"
else
    echo "ğŸ“¥ Installing Ollama..."
    curl -fsSL https://ollama.com/install.sh | sh
    echo "âœ… Ollama installed"
fi

# 2. Pull model
echo "ğŸ“¦ Downloading qwen2.5:14b model..."
ollama pull qwen2.5:14b
echo "âœ… Model downloaded"

# 3. Start service (should auto-start)
echo "ğŸ”„ Checking Ollama service..."
sudo systemctl enable ollama
sudo systemctl start ollama

# 4. Test
echo "ğŸ§ª Testing connection..."
sleep 3
response=$(curl -s http://localhost:11434/api/tags)
if [ ! -z "$response" ]; then
    echo "âœ… Ollama is running!"
    echo "Models installed:"
    echo "$response" | grep -o '"name":"[^"]*"' | cut -d'"' -f4
else
    echo "âŒ Connection failed"
fi

echo ""
echo "ğŸ‰ Setup complete! You can now use Local AI in PMQA app."
```

**à¸§à¸´à¸˜à¸µà¹ƒà¸Šà¹‰:**
```bash
chmod +x ollama-setup.sh
sudo ./ollama-setup.sh
```

---

## ğŸ“Š à¹€à¸›à¸£à¸µà¸¢à¸šà¹€à¸—à¸µà¸¢à¸šà¹‚à¸¡à¹€à¸”à¸¥

| à¹‚à¸¡à¹€à¸”à¸¥ | à¸‚à¸™à¸²à¸” | RAM | à¸ à¸²à¸©à¸²à¹„à¸—à¸¢ | à¸„à¸§à¸²à¸¡à¹€à¸£à¹‡à¸§ | à¹à¸™à¸°à¸™à¸³à¸ªà¸³à¸«à¸£à¸±à¸š |
|-------|------|-----|---------|---------|------------|
| **qwen2.5:14b** | 9 GB | 16 GB | â­â­â­â­â­ | âš¡âš¡ | à¹ƒà¸Šà¹‰à¸‡à¸²à¸™à¸ˆà¸£à¸´à¸‡ (Production) |
| llama3.1:8b | 4.7 GB | 8 GB | â­â­â­ | âš¡âš¡âš¡ | à¸—à¸”à¸ªà¸­à¸š / RAM à¸™à¹‰à¸­à¸¢ |
| mistral:7b | 4.1 GB | 8 GB | â­â­â­â­ | âš¡âš¡âš¡ | à¸ªà¸¡à¸”à¸¸à¸¥à¸”à¸µ |
| llama3.1:70b | 40 GB | 64 GB | â­â­â­â­â­ | âš¡ | Server à¹à¸£à¸‡à¸¡à¸²à¸ |

---

## ğŸ¯ Next Steps

à¸«à¸¥à¸±à¸‡à¸•à¸´à¸”à¸•à¸±à¹‰à¸‡à¹€à¸ªà¸£à¹‡à¸ˆ:

1. âœ… Login à¹€à¸‚à¹‰à¸² PMQA Web App
2. âœ… à¹„à¸› Settings â†’ AI Provider
3. âœ… à¹€à¸¥à¸·à¸­à¸ "Local AI (Ollama)"
4. âœ… à¸—à¸”à¸ªà¸­à¸š Chat with PMQA Rules
5. âœ… à¸—à¸”à¸ªà¸­à¸š Smart Evidence Tagging

---

## ğŸ“ à¸•à¸´à¸”à¸•à¹ˆà¸­ Support

à¸«à¸²à¸à¸¡à¸µà¸›à¸±à¸à¸«à¸²:
- ğŸ“– à¸­à¹ˆà¸²à¸™ Troubleshooting à¸”à¹‰à¸²à¸™à¸šà¸™
- ğŸŒ à¸”à¸¹ Ollama Docs: https://ollama.com/docs
- ğŸ’¬ à¸•à¸´à¸”à¸•à¹ˆà¸­ Admin à¸‚à¸­à¸‡à¸£à¸°à¸šà¸š

---

**à¸ªà¸¸à¸”à¸—à¹‰à¸²à¸¢:** à¸¢à¸´à¸™à¸”à¸µà¸•à¹‰à¸­à¸™à¸£à¸±à¸šà¸ªà¸¹à¹ˆ Local AI! ğŸ‰ à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸‚à¸­à¸‡à¸„à¸¸à¸“à¸ˆà¸°à¸›à¸¥à¸­à¸”à¸ à¸±à¸¢à¹à¸¥à¸°à¹„à¸¡à¹ˆà¸­à¸­à¸à¸ˆà¸²à¸ Server à¸­à¸µà¸à¸•à¹ˆà¸­à¹„à¸› ğŸ›¡ï¸
